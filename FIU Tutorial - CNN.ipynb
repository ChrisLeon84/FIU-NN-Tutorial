{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40eb45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d85534-c562-4207-a3f0-7f124e7a1367",
   "metadata": {},
   "source": [
    "This is tutorial we'll learn how to use CNN's. The data is an MHD simulations, specifically the Orszang-Tang vortex. \n",
    "\n",
    "The network will be used to predict the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3813a7c-5aa9-4bd8-aaf2-9e64fcee0b7a",
   "metadata": {},
   "source": [
    "First, we'll load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc692cf-90b9-4d26-88fa-d03ae93c16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "rho = np.load('rho.npy')\n",
    "\n",
    "# Create the two datasets, one as input to network, one as output\n",
    "# Create dummy index: needed for \"color channel\" in Tensorflow\n",
    "rho_prev = rho[:-1, :, :, np.newaxis]\n",
    "rho_next = rho[1:, :, :, np.newaxis] \n",
    "\n",
    "# Training examples\n",
    "n_train = 1000\n",
    "\n",
    "#Training set (first 1000 time steps)\n",
    "X_train = rho_prev[:n_train]\n",
    "y_train = rho_next[:n_train]\n",
    "\n",
    "#Test set (everyting after)\n",
    "X_test = rho_prev[n_train:]\n",
    "y_test = rho_next[n_train:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09a4e5ef-1119-42e4-a968-0ac905fb1518",
   "metadata": {},
   "source": [
    "Next, we'll create the CNN encoder-decoder.\n",
    "\n",
    "The CONV2D combines convolution and sigma layers. If we have:\n",
    "\n",
    "    x = Conv2D(F, (3, 3), activation='relu', padding='same', \n",
    "                  strides = 2)(inputs)\n",
    "\n",
    "The F signifies the number of filters. The (3,3) indicates the size of filter. The other inputs are exactly what they seem: use a 'relu' activation function, pad, and take a stride length of 2.   \n",
    "\n",
    "The Conv2DTranspose works like CONV3D in reverse.\n",
    "\n",
    "![ChessUrl](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kOThnLR8Fge_AJcHrkR3dg.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce194eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 4)         40        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 8)           296       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 2)           146       \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 16, 16, 8)        152       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 4)        292       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 1)         37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 963\n",
      "Trainable params: 963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 6s 6ms/step - loss: 0.0049\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0014\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5557e-04\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.2489e-04\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 2.7119e-04\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.3392e-04\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.0699e-04\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.8814e-04\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.7576e-04\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.6683e-04\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.5980e-04\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.5411e-04\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.4843e-04\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.4314e-04\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.3852e-04\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.3380e-04\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.2983e-04\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.2502e-04\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.2049e-04\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.1554e-04\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.1071e-04\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.0606e-04\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.0092e-04\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 9.7001e-05\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 9.3683e-05\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 9.0248e-05\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 8.7667e-05\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 8.5203e-05\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 8.3082e-05\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 8.0515e-05\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.9206e-05\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.7559e-05\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.5878e-05\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.4456e-05\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 7.3170e-05\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.1836e-05\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.0576e-05\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.0035e-05\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.8476e-05\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 6.7631e-05\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 6.7063e-05\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.5582e-05\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.5160e-05\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.3756e-05\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.3913e-05\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.2842e-05\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.2327e-05\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.1839e-05\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 6.1073e-05\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 6.1073e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rho model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rho model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Number of filters in first convolutional layer\n",
    "F = 4\n",
    "\n",
    "# Number of filters in the latent space\n",
    "lat_dim = 2\n",
    "\n",
    "def create_encoder_decoder(input_shape=(32, 32, 1)):\n",
    "    ####### Encoder #######\n",
    "    \n",
    "    # 32x32x1 going in\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(F, (3, 3), activation='relu', padding='same', \n",
    "              strides = 2)(inputs)\n",
    "    \n",
    "    # 16x16xF\n",
    "    x = Conv2D(2*F, (3, 3), activation='relu', padding='same',\n",
    "              strides = 2)(x)\n",
    "    \n",
    "    \n",
    "    ####### Latent Space #######\n",
    "    # 8x8xF  =>  latent space: 8x8xlatent_dim\n",
    "    x = Conv2D(lat_dim, (3, 3),  padding='same')(x)\n",
    "\n",
    "\n",
    "    \n",
    "    ###### Decoder ########\n",
    "    # 8x8xlatent_dim\n",
    "    x = Conv2DTranspose(2*F, (3, 3), activation='relu', strides = 2, \n",
    "                        padding='same')(x)\n",
    "    \n",
    "    # 16x16x2F\n",
    "    x = Conv2DTranspose(F, (3, 3), activation='relu', strides = 2, \n",
    "                        padding='same')(x)\n",
    "    # 32x32xF => 32x32x1\n",
    "    outputs = Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create Model\n",
    "rho_model = create_encoder_decoder()\n",
    "\n",
    "# Print out a summary of the model\n",
    "rho_model.summary()\n",
    "\n",
    "# Compile the model (you can adjust the optimizer and loss function as needed)\n",
    "rho_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit data. Specify batch size and number of epochs to sun.\n",
    "history = rho_model.fit(X_train, y_train,\n",
    "                                batch_size= 4,\n",
    "                                epochs=50\n",
    "                                )\n",
    "\n",
    "# Save Model\n",
    "rho_model.save('rho model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8b376-d413-4b36-a626-6e1b99887a19",
   "metadata": {},
   "source": [
    "Now, we plot the results and see how well it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d85dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m rho_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.45\u001b[39m\n\u001b[0;32m      4\u001b[0m err_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.09\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mn_steps\u001b[49m, \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get the True Rho\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_steps' is not defined"
     ]
    }
   ],
   "source": [
    "# Scales for plots\n",
    "rho_min = 0\n",
    "rho_max = 0.45\n",
    "err_max = 0.09\n",
    "\n",
    "for t in range(1, n_steps, 100):\n",
    "    plt.figure(t+1, figsize=(15,4))\n",
    "    \n",
    "    # Get the True Rho\n",
    "    truth = rho_next[t, :, :, 0]\n",
    "    \n",
    "    # Get Model Prediciton\n",
    "    \"\"\"\n",
    "    Since the model trained 4 index array (time, x, y, dummy) best\n",
    "    to use 4 index array. Hence the t:t+1 term.\n",
    "    Then redefine so that array is 2D\n",
    "    \"\"\" \n",
    "    model_prediction = rho_model(rho_prev[t:t+1,:,:,:])\n",
    "    model_prediction = model_prediction[0,:,:,0]\n",
    "    \n",
    "    ### Plot 1\n",
    "    plt.subplot(1,3, 1)\n",
    "    plt.imshow(truth.T,\n",
    "               vmin = rho_min, vmax = rho_max,\n",
    "               extent = [0, 1, 0, 1])\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')\n",
    "    plt.title('Truth')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    ### Plot 2\n",
    "    plt.subplot(1,3, 2)\n",
    "    plt.imshow(np.array(model_prediction).T,\n",
    "               vmin = rho_min, vmax = rho_max,\n",
    "               extent = [0, 1, 0, 1])\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.title('Model Prediction')\n",
    "\n",
    "    \n",
    "    ### Plot 3\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(truth.T - np.array(model_prediction).T,\n",
    "               vmin = -err_max, vmax = err_max,\n",
    "               extent = [0, 1, 0, 1],\n",
    "               cmap = 'gist_rainbow')\n",
    "    plt.colorbar()\n",
    "    plt.title('Error')\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    plt.suptitle(f'time = {t}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Get average error over average rho    \n",
    "\n",
    "# Initiate\n",
    "rel_err = np.zeros(n_steps-1)\n",
    "\n",
    "for t in range(n_steps-1):\n",
    "    # Get the True Rho\n",
    "    truth = rho_next[t, :, :, 0]\n",
    "    \n",
    "    # Average rho value at t\n",
    "    ave_rho_t = np.mean(truth)\n",
    "    \n",
    "    # Get Model Prediciton\n",
    "    model_prediction = rho_model(rho_prev[t:t+1,:,:,:])\n",
    "    model_prediction = model_prediction[0,:,:,0]\n",
    "    \n",
    "    # Get the average absolte error\n",
    "    ave_abs_err = np.abs(np.mean(truth -model_prediction))\n",
    "    \n",
    "    rel_err[t] = ave_abs_err/ave_rho_t\n",
    "    \n",
    "    \n",
    "time = np.linspace(0,0.75, n_steps-1)\n",
    "\n",
    "plt.figure(1300, figsize=(6,4))\n",
    "plt.plot(time,100*rel_err)\n",
    "plt.title('Relative Error')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$\\langle|\\rho - \\hat{\\rho}| \\rangle / \\langle \\rho \\rangle$  (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091905d-396c-4bd9-9b58-0086706806a5",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "1. Now, see how the CNN does on the test set. Make it so that it takes in the last training set data, predicts the next time step and feeds the predictions into itself. See how it compres to the real thing.\n",
    "\n",
    "Project idea: Predicting fluid evoltuion on the entire space can be computationally expensive. Some have looked into rather mapping the fluid state onto a latent space and looking at the evolution there.\n",
    "\n",
    "Create a variational autoencoder (look it up) for the training data. Now, create another model that tracks how the motion is occuring in the latent space (LSTM). See how does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35eb18-87c2-44e7-a2cb-6f7b3a738901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4fb6f-653d-4e85-9b8c-5867b46bb408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
